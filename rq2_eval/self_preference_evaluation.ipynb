{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e91d04a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\peera\\project\\submission_arr202510\\software\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from utils.benchmark_evaluator import SelfPreferenceBiasEvaluator\n",
    "from utils.load_cultural_dataset import CulturalBiasDataset\n",
    "import pickle\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from colpali_engine.models import ColPaliProcessor, ColQwen2Processor, ColQwen2_5_Processor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8165d96",
   "metadata": {},
   "source": [
    "## Setup paths and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c00a2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "current_path = os.getcwd()\n",
    "# BENCHMARK_PATH = os.path.join(current_path, 'benchmarks', 'xcm-bench.csv')\n",
    "# DATASET_PATH =  os.path.join(current_path, \"datasets\", \"tierone003_deduplicated_and_renamed\")\n",
    "DATASET_PATH =  'Chula-AI/association_bias_benchmark'\n",
    "EMBEDDING_DIR_PATH = os.path.join(current_path, 'embeddings')\n",
    "\n",
    "def find_model_type(model_name):\n",
    "    \"\"\"Determine model type from model name\"\"\"\n",
    "    model_name = model_name.lower()\n",
    "    if 'clip' in model_name:\n",
    "        return 'clip'\n",
    "    elif 'xlm' in model_name:\n",
    "        return 'clip'\n",
    "    elif 'jina' in model_name:\n",
    "        return 'clip'\n",
    "    elif 'siglip2' in model_name:\n",
    "        return 'siglip2'\n",
    "    elif 'gme' in model_name:\n",
    "        return 'gme'\n",
    "    elif 'colqwen2.5' in model_name:\n",
    "        return 'colqwen2.5'\n",
    "    elif 'colqwen2' in model_name:\n",
    "        return 'colqwen2'\n",
    "    elif 'colpali' in model_name:\n",
    "        return 'colpali'\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model type for model name: {model_name}\")\n",
    "\n",
    "# Get list of embedding files\n",
    "embedding_files = os.listdir(EMBEDDING_DIR_PATH)\n",
    "embeddings_list = []\n",
    "\n",
    "# Process embedding files\n",
    "for file_name in embedding_files:\n",
    "    # Skip .gitkeep files\n",
    "    if file_name == '.gitkeep':\n",
    "        continue\n",
    "    model_name = file_name[22:-12].replace('_', '-')  # Extract model name from filename\n",
    "    model_type = find_model_type(model_name)\n",
    "    embeddings_list.append({\n",
    "        'model_name': model_name,\n",
    "        'model_type': model_type,\n",
    "        'embedding_file_name': file_name\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0222f96",
   "metadata": {},
   "source": [
    "## Evaluate all embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9662a16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since Chula-AI/association_bias_benchmark couldn't be found on the Hugging Face Hub\n",
      "WARNING:datasets.load:Using the latest cached version of the dataset since Chula-AI/association_bias_benchmark couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'image_metadata' at C:\\Users\\peera\\.cache\\huggingface\\datasets\\Chula-AI___association_bias_benchmark\\image_metadata\\0.0.0\\c08ef2c5953051f6b3dac3fd6cd014cd13275f5b (last modified on Fri Oct 17 01:51:19 2025).\n",
      "WARNING:datasets.packaged_modules.cache.cache:Found the latest cached dataset configuration 'image_metadata' at C:\\Users\\peera\\.cache\\huggingface\\datasets\\Chula-AI___association_bias_benchmark\\image_metadata\\0.0.0\\c08ef2c5953051f6b3dac3fd6cd014cd13275f5b (last modified on Fri Oct 17 01:51:19 2025).\n",
      "Using the latest cached version of the dataset since Chula-AI/association_bias_benchmark couldn't be found on the Hugging Face Hub\n",
      "WARNING:datasets.load:Using the latest cached version of the dataset since Chula-AI/association_bias_benchmark couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'benchmark' at C:\\Users\\peera\\.cache\\huggingface\\datasets\\Chula-AI___association_bias_benchmark\\benchmark\\0.0.0\\c08ef2c5953051f6b3dac3fd6cd014cd13275f5b (last modified on Fri Oct 17 02:10:35 2025).\n",
      "WARNING:datasets.packaged_modules.cache.cache:Found the latest cached dataset configuration 'benchmark' at C:\\Users\\peera\\.cache\\huggingface\\datasets\\Chula-AI___association_bias_benchmark\\benchmark\\0.0.0\\c08ef2c5953051f6b3dac3fd6cd014cd13275f5b (last modified on Fri Oct 17 02:10:35 2025).\n",
      "Processing embeddings:   0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating model: clip-vit-large-patch14\n",
      "Model type: clip\n",
      "Using embedding file: image_text_embeddings_clip-vit-large-patch14_0_11759.pkl\n",
      "file path: c:\\Users\\peera\\project\\submission_arr202510\\software\\rq2_eval\\embeddings\\image_text_embeddings_clip-vit-large-patch14_0_11759.pkl\n",
      "Converted embeddings to dictionary of lists.\n",
      "\n",
      "Evaluating text-to-image...\n",
      "len new bench mark: 11724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11724/11724 [00:05<00:00, 2225.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating image-to-image...\n",
      "len new bench mark: 11724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11724/11724 [00:04<00:00, 2526.53it/s]\n",
      "Processing embeddings:  14%|█▍        | 1/7 [00:28<02:50, 28.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Completed evaluation for model: clip-vit-large-patch14\n",
      "\n",
      "Evaluating model: ColQwen2.5-3b-multilingual-v1.0\n",
      "Model type: colqwen2.5\n",
      "Using embedding file: image_text_embeddings_ColQwen2.5-3b-multilingual-v1.0_0_11759.pkl\n",
      "file path: c:\\Users\\peera\\project\\submission_arr202510\\software\\rq2_eval\\embeddings\\image_text_embeddings_ColQwen2.5-3b-multilingual-v1.0_0_11759.pkl\n",
      "Converted embeddings to dictionary of lists.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating text-to-image...\n",
      "len new bench mark: 11724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11724/11724 [02:59<00:00, 65.48it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating image-to-image...\n",
      "len new bench mark: 11724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11724/11724 [10:44<00:00, 18.18it/s]\n",
      "Processing embeddings:  29%|██▊       | 2/7 [15:56<46:28, 557.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Completed evaluation for model: ColQwen2.5-3b-multilingual-v1.0\n",
      "\n",
      "Evaluating model: colqwen2.5-v0.2\n",
      "Model type: colqwen2.5\n",
      "Using embedding file: image_text_embeddings_colqwen2.5-v0.2_0_11759.pkl\n",
      "file path: c:\\Users\\peera\\project\\submission_arr202510\\software\\rq2_eval\\embeddings\\image_text_embeddings_colqwen2.5-v0.2_0_11759.pkl\n",
      "Converted embeddings to dictionary of lists.\n",
      "\n",
      "Evaluating text-to-image...\n",
      "len new bench mark: 11724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11724/11724 [00:25<00:00, 461.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating image-to-image...\n",
      "len new bench mark: 11724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11724/11724 [00:52<00:00, 224.36it/s]\n",
      "Processing embeddings:  43%|████▎     | 3/7 [17:57<23:53, 358.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Completed evaluation for model: colqwen2.5-v0.2\n",
      "\n",
      "Evaluating model: gme-Qwen2-VL-2B-Instruct\n",
      "Model type: gme\n",
      "Using embedding file: image_text_embeddings_gme-Qwen2-VL-2B-Instruct_0_11759.pkl\n",
      "file path: c:\\Users\\peera\\project\\submission_arr202510\\software\\rq2_eval\\embeddings\\image_text_embeddings_gme-Qwen2-VL-2B-Instruct_0_11759.pkl\n",
      "Converted embeddings to dictionary of lists.\n",
      "\n",
      "Evaluating text-to-image...\n",
      "len new bench mark: 11724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11724/11724 [00:06<00:00, 1750.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating image-to-image...\n",
      "len new bench mark: 11724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11724/11724 [00:05<00:00, 1955.23it/s]\n",
      "Processing embeddings:  57%|█████▋    | 4/7 [18:35<11:35, 231.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Completed evaluation for model: gme-Qwen2-VL-2B-Instruct\n",
      "\n",
      "Evaluating model: jina-embeddings-v4\n",
      "Model type: clip\n",
      "Using embedding file: image_text_embeddings_jina-embeddings-v4_0_11759.pkl\n",
      "file path: c:\\Users\\peera\\project\\submission_arr202510\\software\\rq2_eval\\embeddings\\image_text_embeddings_jina-embeddings-v4_0_11759.pkl\n",
      "Converted embeddings to dictionary of lists.\n",
      "\n",
      "Evaluating text-to-image...\n",
      "len new bench mark: 11724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11724/11724 [00:06<00:00, 1711.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating image-to-image...\n",
      "len new bench mark: 11724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11724/11724 [00:05<00:00, 2156.10it/s]\n",
      "Processing embeddings:  71%|███████▏  | 5/7 [19:12<05:22, 161.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Completed evaluation for model: jina-embeddings-v4\n",
      "\n",
      "Evaluating model: XLM-Roberta-Large-Vit-B-16Plus\n",
      "Model type: clip\n",
      "Using embedding file: image_text_embeddings_XLM-Roberta-Large-Vit-B-16Plus_0_11759.pkl\n",
      "file path: c:\\Users\\peera\\project\\submission_arr202510\\software\\rq2_eval\\embeddings\\image_text_embeddings_XLM-Roberta-Large-Vit-B-16Plus_0_11759.pkl\n",
      "Converted embeddings to dictionary of lists.\n",
      "\n",
      "Evaluating text-to-image...\n",
      "len new bench mark: 11724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11724/11724 [00:06<00:00, 1923.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating image-to-image...\n",
      "len new bench mark: 11724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11724/11724 [00:05<00:00, 2057.16it/s]\n",
      "Processing embeddings:  86%|████████▌ | 6/7 [19:46<01:58, 118.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Completed evaluation for model: XLM-Roberta-Large-Vit-B-16Plus\n",
      "\n",
      "Evaluating model: XLM-Roberta-Large-Vit-L-14\n",
      "Model type: clip\n",
      "Using embedding file: image_text_embeddings_XLM-Roberta-Large-Vit-L-14_0_11759.pkl\n",
      "file path: c:\\Users\\peera\\project\\submission_arr202510\\software\\rq2_eval\\embeddings\\image_text_embeddings_XLM-Roberta-Large-Vit-L-14_0_11759.pkl\n",
      "Converted embeddings to dictionary of lists.\n",
      "\n",
      "Evaluating text-to-image...\n",
      "len new bench mark: 11724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11724/11724 [00:06<00:00, 1799.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating image-to-image...\n",
      "len new bench mark: 11724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11724/11724 [00:07<00:00, 1662.98it/s]\n",
      "Processing embeddings: 100%|██████████| 7/7 [20:23<00:00, 174.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Completed evaluation for model: XLM-Roberta-Large-Vit-L-14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load dataset and benchmark\n",
    "dataset = CulturalBiasDataset(DATASET_PATH)\n",
    "benchmark  = load_dataset(DATASET_PATH, name=\"benchmark\", split='train')\n",
    "# benchmark_df = pd.read_csv(BENCHMARK_PATH)\n",
    "# benchmark  = benchmark_df.to_dict('records')\n",
    "\n",
    "# Process each embedding file\n",
    "for embedding_info in tqdm(embeddings_list, desc=\"Processing embeddings\"):\n",
    "    model_name = embedding_info['model_name']\n",
    "    model_type = embedding_info['model_type']\n",
    "    embedding_file_name = embedding_info['embedding_file_name']\n",
    "\n",
    "    print(f\"\\nEvaluating model: {model_name}\")\n",
    "    print(f\"Model type: {model_type}\")\n",
    "    print(f\"Using embedding file: {embedding_file_name}\")\n",
    "\n",
    "    # Clear previous embeddings to free up memory\n",
    "    if hasattr(dataset, 'embeddings'):\n",
    "        del dataset.embeddings\n",
    "    embeddings = {} # Start with a clean slate\n",
    "    \n",
    "    # Load embeddings into dataset\n",
    "    dataset.import_local_embedding(EMBEDDING_DIR_PATH, embedding_file_name)\n",
    "    embeddings = dataset.embeddings\n",
    "    \n",
    "    # Initialize processor for late interaction models\n",
    "    processor = None\n",
    "    if model_type == 'colpali':\n",
    "        processor = ColPaliProcessor.from_pretrained(\"vidore/colpali-v1.3-merged\")\n",
    "    elif model_type == 'colqwen2':\n",
    "        processor = ColQwen2Processor.from_pretrained(\"vidore/colqwen2-v1.0\")\n",
    "    elif model_type == 'colqwen2.5':\n",
    "        processor = ColQwen2_5_Processor.from_pretrained(\"vidore/colqwen2.5-v0.2\")\n",
    "    \n",
    "    # Create evaluator instance\n",
    "    evaluator = SelfPreferenceBiasEvaluator(\n",
    "        dataset=dataset,\n",
    "        embeddings=embeddings,\n",
    "        model_name=model_name,\n",
    "        model_type=model_type,\n",
    "        processor=processor\n",
    "    )\n",
    "    \n",
    "    # Evaluate both text-to-image and image-to-image experiments\n",
    "    print(\"\\nEvaluating text-to-image...\")\n",
    "    evaluator.evaluate_and_save(benchmark, experiment_type=\"text-to-image\")\n",
    "    \n",
    "    print(\"\\nEvaluating image-to-image...\")\n",
    "    evaluator.evaluate_and_save(benchmark, experiment_type=\"image-to-image\")\n",
    "    \n",
    "    print(f\"\\nCompleted evaluation for model: {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba6e674",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getenv(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842cdaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "image_metadata = load_dataset('Chula-AI/association_bias_benchmark', name=\"image_metadata\", split='train')\n",
    "benchmark_data = load_dataset('Chula-AI/association_bias_benchmark', name=\"benchmark\", split='train')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbed44cd",
   "metadata": {},
   "source": [
    "## Evaluate Single Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d10ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the embedding file to evaluate\n",
    "embedding_file_name = r\"image_text_embeddings_XLM-Roberta-Large-Vit-B-16Plus_0_11759.pkl\"\n",
    "model_name = embedding_file_name[22:-12].replace('_', '-')  # Extract model name from filename\n",
    "model_type = find_model_type(model_name)\n",
    "\n",
    "print(f\"Selected model: {model_name}\")\n",
    "print(f\"Model type: {model_type}\")\n",
    "print(f\"Embedding file: {embedding_file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f35049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dataset and load benchmark\n",
    "dataset = CulturalBiasDataset(DATASET_PATH)\n",
    "benchmark_df = pd.read_csv(BENCHMARK_PATH)\n",
    "benchmark  = benchmark_df.to_dict('records')\n",
    "\n",
    "\n",
    "# Load embedding into dataset\n",
    "dataset.import_local_embedding(EMBEDDING_DIR_PATH, embedding_file_name)\n",
    "embeddings = dataset.embeddings\n",
    "\n",
    "# Initialize processor if needed for late interaction models\n",
    "processor = None\n",
    "if model_type == 'colpali':\n",
    "    processor = ColPaliProcessor.from_pretrained(\"vidore/colpali-v1.3-merged\")\n",
    "elif model_type == 'colqwen2':\n",
    "    processor = ColQwen2Processor.from_pretrained(\"vidore/colqwen2-v1.0\")\n",
    "elif model_type == 'colqwen2.5':\n",
    "    processor = ColQwen2_5_Processor.from_pretrained(\"vidore/colqwen2.5-v0.2\")\n",
    "\n",
    "# Create evaluator instance\n",
    "evaluator = SelfPreferenceBiasEvaluator(\n",
    "    dataset=dataset,\n",
    "    embeddings=embeddings,\n",
    "    model_name=model_name,\n",
    "    model_type=model_type,\n",
    "    processor=processor\n",
    ")\n",
    "\n",
    "# Evaluate both experiments\n",
    "print(\"\\nEvaluating text-to-image...\")\n",
    "text_to_image_results = evaluator.evaluate_and_save(benchmark, experiment_type=\"text-to-image\")\n",
    "\n",
    "print(\"\\nEvaluating image-to-image...\")\n",
    "image_to_image_results = evaluator.evaluate_and_save(benchmark, experiment_type=\"image-to-image\")\n",
    "\n",
    "print(f\"\\nEvaluation completed for model: {model_name}\")\n",
    "\n",
    "# Display summary results\n",
    "print(\"\\nText-to-Image Results:\")\n",
    "print(\"Overall wins:\", text_to_image_results[\"overall_one_hot\"])\n",
    "\n",
    "print(\"\\nImage-to-Image Results:\")\n",
    "print(\"Overall wins:\", image_to_image_results[\"overall_one_hot\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f978765",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.embeddings['image_embedding'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139b0a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.embeddings['image_embedding'][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2ece0e",
   "metadata": {},
   "source": [
    "## Aggregate Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70021ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluation results paths\n",
    "EVALUATION_DIR_PATH = os.path.join(current_path, \"evaluation_results\")\n",
    "EVALUATION_DIR_TEXT2IMG_PATH = os.path.join(EVALUATION_DIR_PATH, 'text-to-image')\n",
    "EVALUATION_DIR_IMG2IMG_PATH = os.path.join(EVALUATION_DIR_PATH, 'image-to-image')\n",
    "\n",
    "def extract_model_from_file_name(file_name):\n",
    "    \"\"\"Extract model name from summary file name\"\"\"\n",
    "    last_dot_index = file_name.rfind('.')\n",
    "    if last_dot_index != -1:\n",
    "        result = file_name[:last_dot_index]\n",
    "    else:\n",
    "        result = file_name\n",
    "    return result.split('_')[-1]\n",
    "\n",
    "def aggregate_evaluation_results(start_with):\n",
    "    \"\"\"Aggregate evaluation results from all models\"\"\"\n",
    "    combined_df = pd.DataFrame()\n",
    "    \n",
    "    # Process image-to-image results\n",
    "    for file in os.listdir(EVALUATION_DIR_IMG2IMG_PATH):\n",
    "        if file.startswith(start_with):\n",
    "            df = pd.read_csv(os.path.join(EVALUATION_DIR_IMG2IMG_PATH, file))\n",
    "            df['model'] = extract_model_from_file_name(file)\n",
    "            df['experiment'] = 'image-to-image'\n",
    "            combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "    \n",
    "    # Process text-to-image results\n",
    "    for file in os.listdir(EVALUATION_DIR_TEXT2IMG_PATH):\n",
    "        if file.startswith(start_with):\n",
    "            df = pd.read_csv(os.path.join(EVALUATION_DIR_TEXT2IMG_PATH, file))\n",
    "            df['model'] = extract_model_from_file_name(file)\n",
    "            df['experiment'] = 'text-to-image'\n",
    "            combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "            \n",
    "    return combined_df\n",
    "\n",
    "# Aggregate results by type\n",
    "print(\"Aggregating results...\")\n",
    "concept_df = aggregate_evaluation_results('concept_summary')\n",
    "country_df = aggregate_evaluation_results('country_summary')\n",
    "overall_df = aggregate_evaluation_results('overall_summary')\n",
    "language_df = aggregate_evaluation_results('language_summary')\n",
    "\n",
    "# Create aggregated directory and save results\n",
    "aggregated_dir = os.path.join(EVALUATION_DIR_PATH, \"aggregated\")\n",
    "os.makedirs(aggregated_dir, exist_ok=True)\n",
    "\n",
    "def save_if_different(df, filepath):\n",
    "    \"\"\"Save DataFrame only if it's different from existing file or file doesn't exist\"\"\"\n",
    "    if os.path.exists(filepath):\n",
    "        existing_df = pd.read_csv(filepath)\n",
    "        if not existing_df.equals(df):\n",
    "            print(f\"Updating {os.path.basename(filepath)} - content has changed\")\n",
    "            df.to_csv(filepath, index=False)\n",
    "        else:\n",
    "            print(f\"Skipping {os.path.basename(filepath)} - content unchanged\")\n",
    "    else:\n",
    "        print(f\"Creating new file {os.path.basename(filepath)}\")\n",
    "        df.to_csv(filepath, index=False)\n",
    "\n",
    "# Save aggregated results\n",
    "print(\"\\nSaving aggregated results...\")\n",
    "save_if_different(concept_df, os.path.join(aggregated_dir, \"concept_aggregated.csv\"))\n",
    "save_if_different(country_df, os.path.join(aggregated_dir, \"country_aggregated.csv\"))\n",
    "save_if_different(overall_df, os.path.join(aggregated_dir, \"overall_aggregated.csv\"))\n",
    "save_if_different(language_df, os.path.join(aggregated_dir, \"language_aggregated.csv\"))\n",
    "\n",
    "print(\"\\nAggregation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bf4862",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e9a9d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
