{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5525eece",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb40bb7",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c498952",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers datasets tqdm pandas colpali-engine multilingual-clip open-clip-torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b2b9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset configuration\n",
    "DATASET_NAME = \"Chula-AI/association_bias_benchmark\"  # Dataset to use\n",
    "EMBEDDING_SAVE_PATH = \"embeddings\"  # Path to save the embeddings\n",
    "\n",
    "# Processing configuration\n",
    "START_INDEX = 0\n",
    "END_INDEX = None  # None means process all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7808a8bb",
   "metadata": {},
   "source": [
    "# Embeddings\n",
    "\n",
    "Expected schema:\n",
    "- concept_id\n",
    "- concept_in_native  \n",
    "- image_id\n",
    "- image_key (for backward compatibility)\n",
    "- index_in_dataset\n",
    "- concept\n",
    "- concept_country\n",
    "- country\n",
    "- title\n",
    "- image_embedding\n",
    "- text_embeddings\n",
    "  - concept_embedding\n",
    "  - translated_concept_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81e41f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to load dataset\n",
    "def load_bias_benchmark_dataset(dataset_name):\n",
    "    \"\"\"\n",
    "    Load Association Bias Benchmark dataset.\n",
    "    \n",
    "    Args:\n",
    "        dataset_name: Name of the dataset on HuggingFace\n",
    "    \n",
    "    Returns:\n",
    "        Dataset with image metadata\n",
    "    \"\"\"\n",
    "    print(f\"Loading dataset: {dataset_name}\")\n",
    "    dataset = load_dataset(dataset_name, name=\"image_metadata\", split=\"train\")\n",
    "    print(f\"Dataset loaded with {len(dataset)} entries\")\n",
    "    return dataset\n",
    "\n",
    "def concatenate_tensors(tensor_list):\n",
    "    \"\"\"\n",
    "    Concatenates a list of tensors along dimension 0.\n",
    "    Used for Jina embeddings.\n",
    "    \"\"\"\n",
    "    if len(tensor_list) == 1:\n",
    "        return tensor_list[0].unsqueeze(0)\n",
    "    buffed_list = []\n",
    "    for tensor in tensor_list:\n",
    "        buffed_list.append(tensor.unsqueeze(0))\n",
    "    return torch.cat(buffed_list, dim=0)\n",
    "\n",
    "print(\"Helper functions loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e19bb3",
   "metadata": {},
   "source": [
    "## CLIP Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a517a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLIP imports\n",
    "from transformers import AutoModel, AutoProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98a4910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dropdown widget for CLIP model selection\n",
    "clip_model_options = [\n",
    "    (\"OpenAI CLIP ViT-Large\", \"openai/clip-vit-large-patch14\"),\n",
    "    (\"Chinese CLIP ViT-Large\", \"OFA-Sys/chinese-clip-vit-large-patch14\")\n",
    "]\n",
    "\n",
    "clip_model_dropdown = widgets.Dropdown(\n",
    "    options=clip_model_options,\n",
    "    value=\"openai/clip-vit-large-patch14\",\n",
    "    description='CLIP Model:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "def on_clip_model_change(change):\n",
    "    global clip_model_name, clip_model_name_part\n",
    "    clip_model_name = change['new']\n",
    "    clip_model_name_part = clip_model_name.split(\"/\")[-1]\n",
    "    print(f\"Selected CLIP model: {clip_model_name}\")\n",
    "\n",
    "clip_model_dropdown.observe(on_clip_model_change, names='value')\n",
    "\n",
    "# Initialize variables\n",
    "clip_model_name = clip_model_dropdown.value\n",
    "clip_model_name_part = clip_model_name.split(\"/\")[-1]\n",
    "\n",
    "display(clip_model_dropdown)\n",
    "print(f\"Initial CLIP model: {clip_model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60182a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CLIP model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "clip_model = AutoModel.from_pretrained(clip_model_name).to(device)\n",
    "clip_processor = AutoProcessor.from_pretrained(clip_model_name)\n",
    "config = clip_model.config\n",
    "MAX_LENGTH = config.text_config.max_position_embeddings\n",
    "\n",
    "print(f\"CLIP model loaded: {clip_model_name}\")\n",
    "print(f\"Maximum text length: {MAX_LENGTH} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e71a8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset for CLIP processing\n",
    "dataset = load_bias_benchmark_dataset(DATASET_NAME)\n",
    "\n",
    "eval_size = min(np.inf, len(dataset)) if END_INDEX is None else min(END_INDEX, len(dataset))\n",
    "eval_dataset = dataset.select(range(eval_size))\n",
    "\n",
    "embeddings_list = []\n",
    "\n",
    "# Setup save path\n",
    "os.makedirs(EMBEDDING_SAVE_PATH, exist_ok=True)\n",
    "save_path = os.path.join(EMBEDDING_SAVE_PATH, f\"image_text_embeddings_{clip_model_name_part}_{START_INDEX}_{eval_size}.pkl\")\n",
    "\n",
    "print(f\"Processing entries from index {START_INDEX} to {eval_size}...\")\n",
    "print(f\"Total dataset size: {len(eval_dataset)}\")\n",
    "\n",
    "# Process entries\n",
    "for i in tqdm(range(START_INDEX, min(eval_size, len(eval_dataset))), desc=\"Processing CLIP embeddings\"):\n",
    "    current_entry = eval_dataset[i]\n",
    "    \n",
    "    # Initialize entry with expected schema\n",
    "    entry_embeddings = {\n",
    "        'concept_id': current_entry.get('concept_id', None),\n",
    "        'concept_in_native': current_entry.get('concept_in_native'),\n",
    "        'image_id': current_entry.get('image_id', None),\n",
    "        'image_key': current_entry.get('image_id', None),\n",
    "        'index_in_dataset': i,\n",
    "        'concept': current_entry.get('concept', None),\n",
    "        'concept_country': current_entry.get('concept_country', None),\n",
    "        'country': current_entry.get('country', None),\n",
    "        'title': current_entry.get('title', None),\n",
    "        'image_embedding': None,\n",
    "        'text_embeddings': {}\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            # 1. Get image embedding\n",
    "            if current_entry.get('image') is not None:\n",
    "                image_inputs = clip_processor(images=current_entry['image'], return_tensors=\"pt\")\n",
    "                image_inputs = {k: v.to(device) for k, v in image_inputs.items()}\n",
    "                image_outputs = clip_model.get_image_features(**image_inputs)\n",
    "                image_features = image_outputs / image_outputs.norm(dim=-1, keepdim=True)\n",
    "                entry_embeddings['image_embedding'] = image_features.cpu().numpy()\n",
    "            else:\n",
    "                print(f\"Warning: No image found for entry index {i}\")\n",
    "            \n",
    "            # 2. Get text embeddings\n",
    "            if current_entry.get('concept'):\n",
    "                concept_text = str(current_entry['concept'])\n",
    "                text_inputs = clip_processor(\n",
    "                    text=concept_text,\n",
    "                    return_tensors=\"pt\",\n",
    "                    padding=True,\n",
    "                    truncation=True,\n",
    "                    max_length=MAX_LENGTH\n",
    "                )\n",
    "                text_inputs = {k: v.to(device) for k, v in text_inputs.items()}\n",
    "                text_outputs = clip_model.get_text_features(**text_inputs)\n",
    "                text_features = text_outputs / text_outputs.norm(dim=-1, keepdim=True)\n",
    "                entry_embeddings['text_embeddings']['concept_embedding'] = text_features.cpu().numpy()\n",
    "            \n",
    "            if current_entry.get('concept_in_native'):\n",
    "                native_text = str(current_entry['concept_in_native'])\n",
    "                text_inputs = clip_processor(\n",
    "                    text=native_text,\n",
    "                    return_tensors=\"pt\",\n",
    "                    padding=True,\n",
    "                    truncation=True,\n",
    "                    max_length=MAX_LENGTH\n",
    "                )\n",
    "                text_inputs = {k: v.to(device) for k, v in text_inputs.items()}\n",
    "                text_outputs = clip_model.get_text_features(**text_inputs)\n",
    "                text_features = text_outputs / text_outputs.norm(dim=-1, keepdim=True)\n",
    "                entry_embeddings['text_embeddings']['translated_concept_embedding'] = text_features.cpu().numpy()\n",
    "                \n",
    "        embeddings_list.append(entry_embeddings)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing entry {i}: {str(e)}\")\n",
    "        entry_embeddings['error'] = str(e)\n",
    "        embeddings_list.append(entry_embeddings)\n",
    "    \n",
    "    # Clean up GPU memory periodically\n",
    "    if i % 100 == 0:\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# Final cleanup\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Save embeddings\n",
    "print(f\"Saving embeddings to: {save_path}\")\n",
    "with open(save_path, 'wb') as f:\n",
    "    pickle.dump(embeddings_list, f)\n",
    "\n",
    "print(f\"Processing complete. Embeddings saved to: {save_path}\")\n",
    "print(f\"Total entries processed: {len(embeddings_list)}\")\n",
    "\n",
    "# Print summary\n",
    "successful_image_embeddings = sum(1 for entry in embeddings_list if entry.get('image_embedding') is not None)\n",
    "successful_text_embeddings = sum(1 for entry in embeddings_list if entry.get('text_embeddings') and len(entry['text_embeddings']) > 0)\n",
    "\n",
    "print(f\"Successful image embeddings: {successful_image_embeddings}/{len(embeddings_list)}\")\n",
    "print(f\"Successful text embeddings: {successful_text_embeddings}/{len(embeddings_list)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319b274b",
   "metadata": {},
   "source": [
    "## ColQwen Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1dbc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ColQwen imports\n",
    "from colpali_engine.models import ColQwen2_5, ColQwen2_5_Processor, ColQwen2, ColQwen2Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656f0e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dropdown widget for ColQwen model selection\n",
    "colqwen_model_options = [\n",
    "    (\"ColQwen2.5-3b-multilingual\", \"Metric-AI/ColQwen2.5-3b-multilingual-v1.0\"),\n",
    "    (\"ColQwen2.5-7b-multilingual\", \"Metric-AI/ColQwen2.5-7b-multilingual-v1.0\"),\n",
    "    (\"colqwen2.5-v0.2\", \"vidore/colqwen2.5-v0.2\"),\n",
    "    (\"colqwen2-v1.0\", \"vidore/colqwen2-v1.0\")\n",
    "]\n",
    "\n",
    "colqwen_model_dropdown = widgets.Dropdown(\n",
    "    options=colqwen_model_options,\n",
    "    value=\"Metric-AI/ColQwen2.5-3b-multilingual-v1.0\",\n",
    "    description='ColQwen Model:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "def on_colqwen_model_change(change):\n",
    "    global colqwen_model_name, colqwen_model_name_part\n",
    "    colqwen_model_name = change['new']\n",
    "    colqwen_model_name_part = colqwen_model_name.split(\"/\")[-1]\n",
    "    print(f\"Selected ColQwen model: {colqwen_model_name}\")\n",
    "\n",
    "colqwen_model_dropdown.observe(on_colqwen_model_change, names='value')\n",
    "\n",
    "# Initialize variables\n",
    "colqwen_model_name = colqwen_model_dropdown.value\n",
    "colqwen_model_name_part = colqwen_model_name.split(\"/\")[-1]\n",
    "\n",
    "display(colqwen_model_dropdown)\n",
    "print(f\"Initial ColQwen model: {colqwen_model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc264456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ColQwen model\n",
    "if \"colqwen2.5\" in colqwen_model_name.lower():\n",
    "    colqwen_processor = ColQwen2_5_Processor.from_pretrained(colqwen_model_name)\n",
    "    colqwen_model = ColQwen2_5.from_pretrained(\n",
    "        colqwen_model_name,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"cuda:0\"\n",
    "    ).eval()\n",
    "else:\n",
    "    colqwen_processor = ColQwen2Processor.from_pretrained(colqwen_model_name)\n",
    "    colqwen_model = ColQwen2.from_pretrained(\n",
    "        colqwen_model_name,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"cuda:0\"\n",
    "    ).eval()\n",
    "\n",
    "print(f\"ColQwen model loaded: {colqwen_model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6392d3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset for ColQwen processing\n",
    "dataset = load_bias_benchmark_dataset(DATASET_NAME)\n",
    "\n",
    "eval_size = min(np.inf, len(dataset)) if END_INDEX is None else min(END_INDEX, len(dataset))\n",
    "eval_dataset = dataset.select(range(eval_size))\n",
    "\n",
    "embeddings_list = []\n",
    "\n",
    "# Setup save path\n",
    "save_path = os.path.join(EMBEDDING_SAVE_PATH, f\"image_text_embeddings_{colqwen_model_name_part}_{START_INDEX}_{eval_size}.pkl\")\n",
    "\n",
    "print(f\"Processing entries from index {START_INDEX} to {eval_size}...\")\n",
    "\n",
    "# Process entries\n",
    "for i in tqdm(range(START_INDEX, min(eval_size, len(eval_dataset))), desc=\"Processing ColQwen embeddings\"):\n",
    "    current_entry = eval_dataset[i]\n",
    "    \n",
    "    # Initialize entry with expected schema\n",
    "    entry_embeddings = {\n",
    "        'concept_id': current_entry.get('concept_id', None),\n",
    "        'concept_in_native': current_entry.get('concept_in_native'),\n",
    "        'image_id': current_entry.get('image_id', None),\n",
    "        'image_key': current_entry.get('image_id', None),\n",
    "        'index_in_dataset': i,\n",
    "        'concept': current_entry.get('concept', None),\n",
    "        'concept_country': current_entry.get('concept_country', None),\n",
    "        'country': current_entry.get('country', None),\n",
    "        'title': current_entry.get('title', None),\n",
    "        'image_embedding': None,\n",
    "        'text_embeddings': {}\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            # Get text embeddings (ColQwen typically focuses on text)\n",
    "            if current_entry.get('concept'):\n",
    "                batch_queries = colqwen_processor.process_queries([current_entry['concept']]).to(colqwen_model.device)\n",
    "                concept_text_embeddings = colqwen_model(**batch_queries)\n",
    "                entry_embeddings['text_embeddings']['concept_embedding'] = concept_text_embeddings.cpu().float().numpy()\n",
    "            \n",
    "            if current_entry.get('concept_in_native'):\n",
    "                native_queries = colqwen_processor.process_queries([current_entry['concept_in_native']]).to(colqwen_model.device)\n",
    "                native_embeddings = colqwen_model(**native_queries)\n",
    "                entry_embeddings['text_embeddings']['translated_concept_embedding'] = native_embeddings.cpu().float().numpy()\n",
    "                \n",
    "        embeddings_list.append(entry_embeddings)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing entry {i}: {str(e)}\")\n",
    "        entry_embeddings['error'] = str(e)\n",
    "        embeddings_list.append(entry_embeddings)\n",
    "\n",
    "# Final cleanup\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Save embeddings\n",
    "print(f\"Saving embeddings to: {save_path}\")\n",
    "with open(save_path, 'wb') as f:\n",
    "    pickle.dump(embeddings_list, f)\n",
    "\n",
    "print(f\"Processing complete. Total entries processed: {len(embeddings_list)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9157c11e",
   "metadata": {},
   "source": [
    "## GME Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef47fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GME imports\n",
    "from transformers import AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034ca069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dropdown widget for GME model selection\n",
    "gme_model_options = [\n",
    "    (\"GME-Qwen2-VL-2B\", \"Alibaba-NLP/gme-Qwen2-VL-2B-Instruct\"),\n",
    "    (\"GME-Qwen2-VL-7B\", \"Alibaba-NLP/gme-Qwen2-VL-7B-Instruct\")\n",
    "]\n",
    "\n",
    "gme_model_dropdown = widgets.Dropdown(\n",
    "    options=gme_model_options,\n",
    "    value=\"Alibaba-NLP/gme-Qwen2-VL-7B-Instruct\",\n",
    "    description='GME Model:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "def on_gme_model_change(change):\n",
    "    global gme_model_name, gme_model_name_part\n",
    "    gme_model_name = change['new']\n",
    "    gme_model_name_part = gme_model_name.split(\"/\")[-1]\n",
    "    print(f\"Selected GME model: {gme_model_name}\")\n",
    "\n",
    "gme_model_dropdown.observe(on_gme_model_change, names='value')\n",
    "\n",
    "# Initialize variables\n",
    "gme_model_name = gme_model_dropdown.value\n",
    "gme_model_name_part = gme_model_name.split(\"/\")[-1]\n",
    "\n",
    "display(gme_model_dropdown)\n",
    "print(f\"Initial GME model: {gme_model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cc99ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GME model\n",
    "gme = AutoModel.from_pretrained(\n",
    "    gme_model_name,\n",
    "    torch_dtype=\"float16\",\n",
    "    device_map='cuda',\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "print(f\"GME model loaded: {gme_model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13ac0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset for GME processing\n",
    "dataset = load_bias_benchmark_dataset(DATASET_NAME)\n",
    "\n",
    "eval_size = min(np.inf, len(dataset)) if END_INDEX is None else min(END_INDEX, len(dataset))\n",
    "eval_dataset = dataset.select(range(eval_size))\n",
    "\n",
    "embeddings_list = []\n",
    "\n",
    "# Setup save path\n",
    "save_path = os.path.join(EMBEDDING_SAVE_PATH, f\"image_text_embeddings_{gme_model_name_part}_{START_INDEX}_{eval_size}.pkl\")\n",
    "\n",
    "print(f\"Processing entries from index {START_INDEX} to {eval_size}...\")\n",
    "\n",
    "# Process entries\n",
    "for i in tqdm(range(START_INDEX, min(eval_size, len(eval_dataset))), desc=\"Processing GME embeddings\"):\n",
    "    current_entry = eval_dataset[i]\n",
    "    \n",
    "    # Initialize entry with expected schema\n",
    "    entry_embeddings = {\n",
    "        'concept_id': current_entry.get('concept_id', None),\n",
    "        'concept_in_native': current_entry.get('concept_in_native'),\n",
    "        'image_id': current_entry.get('image_id', None),\n",
    "        'image_key': current_entry.get('image_id', None),\n",
    "        'index_in_dataset': i,\n",
    "        'concept': current_entry.get('concept', None),\n",
    "        'concept_country': current_entry.get('concept_country', None),\n",
    "        'country': current_entry.get('country', None),\n",
    "        'title': current_entry.get('title', None),\n",
    "        'image_embedding': None,\n",
    "        'text_embeddings': {}\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            # Get text embeddings (GME focuses on text for this use case)\n",
    "            if current_entry.get('concept'):\n",
    "                concept_text_embeddings = gme.get_text_embeddings(texts=[current_entry['concept']])\n",
    "                entry_embeddings['text_embeddings']['concept_embedding'] = concept_text_embeddings.cpu().numpy()\n",
    "            \n",
    "            if current_entry.get('concept_in_native'):\n",
    "                native_embeddings = gme.get_text_embeddings(texts=[current_entry['concept_in_native']])\n",
    "                entry_embeddings['text_embeddings']['translated_concept_embedding'] = native_embeddings.cpu().numpy()\n",
    "                \n",
    "        embeddings_list.append(entry_embeddings)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing entry {i}: {str(e)}\")\n",
    "        entry_embeddings['error'] = str(e)\n",
    "        embeddings_list.append(entry_embeddings)\n",
    "\n",
    "# Final cleanup\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Save embeddings\n",
    "print(f\"Saving embeddings to: {save_path}\")\n",
    "with open(save_path, 'wb') as f:\n",
    "    pickle.dump(embeddings_list, f)\n",
    "\n",
    "print(f\"Processing complete. Total entries processed: {len(embeddings_list)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a885ddb",
   "metadata": {},
   "source": [
    "## Jina Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012b9204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jina imports\n",
    "from transformers import AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b027bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jina model configuration\n",
    "jina_model_name = 'jinaai/jina-embeddings-v4'\n",
    "jina_model_name_part = jina_model_name.split(\"/\")[-1]\n",
    "\n",
    "print(f\"Using Jina model: {jina_model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d782f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Jina model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "jina_model = AutoModel.from_pretrained(\n",
    "    jina_model_name,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "jina_model.to(device)\n",
    "\n",
    "print(f\"Jina model loaded: {jina_model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778dd8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset for Jina processing\n",
    "dataset = load_bias_benchmark_dataset(DATASET_NAME)\n",
    "\n",
    "eval_size = min(np.inf, len(dataset)) if END_INDEX is None else min(END_INDEX, len(dataset))\n",
    "eval_dataset = dataset.select(range(eval_size))\n",
    "\n",
    "embeddings_list = []\n",
    "\n",
    "# Setup save path\n",
    "save_path = os.path.join(EMBEDDING_SAVE_PATH, f\"image_text_embeddings_{jina_model_name_part}_{START_INDEX}_{eval_size}.pkl\")\n",
    "\n",
    "print(f\"Processing entries from index {START_INDEX} to {eval_size}...\")\n",
    "\n",
    "# Process entries\n",
    "for i in tqdm(range(START_INDEX, min(eval_size, len(eval_dataset))), desc=\"Processing Jina embeddings\"):\n",
    "    current_entry = eval_dataset[i]\n",
    "    \n",
    "    # Initialize entry with expected schema\n",
    "    entry_embeddings = {\n",
    "        'concept_id': current_entry.get('concept_id', None),\n",
    "        'concept_in_native': current_entry.get('concept_in_native'),\n",
    "        'image_id': current_entry.get('image_id', None),\n",
    "        'image_key': current_entry.get('image_id', None),\n",
    "        'index_in_dataset': i,\n",
    "        'concept': current_entry.get('concept', None),\n",
    "        'concept_country': current_entry.get('concept_country', None),\n",
    "        'country': current_entry.get('country', None),\n",
    "        'title': current_entry.get('title', None),\n",
    "        'image_embedding': None,\n",
    "        'text_embeddings': {}\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            # 1. Get image embedding\n",
    "            if current_entry.get('image') is not None:\n",
    "                image_embeddings = jina_model.encode_image(\n",
    "                    images=current_entry['image'],\n",
    "                    task=\"retrieval\",\n",
    "                )\n",
    "                image_embeddings = concatenate_tensors(image_embeddings)\n",
    "                entry_embeddings['image_embedding'] = image_embeddings.cpu().numpy()\n",
    "            else:\n",
    "                print(f\"Warning: No image found for entry index {i}\")\n",
    "            \n",
    "            # 2. Get text embeddings\n",
    "            if current_entry.get('concept'):\n",
    "                concept_embeddings = jina_model.encode_text(\n",
    "                    texts=[current_entry['concept']],\n",
    "                    task=\"retrieval\",\n",
    "                )\n",
    "                concept_embeddings = concatenate_tensors(concept_embeddings)\n",
    "                entry_embeddings['text_embeddings']['concept_embedding'] = concept_embeddings.cpu().numpy()\n",
    "            \n",
    "            if current_entry.get('concept_in_native'):\n",
    "                native_embeddings = jina_model.encode_text(\n",
    "                    texts=[current_entry['concept_in_native']],\n",
    "                    task=\"retrieval\",\n",
    "                )\n",
    "                native_embeddings = concatenate_tensors(native_embeddings)\n",
    "                entry_embeddings['text_embeddings']['translated_concept_embedding'] = native_embeddings.cpu().numpy()\n",
    "                \n",
    "        embeddings_list.append(entry_embeddings)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing entry {i}: {str(e)}\")\n",
    "        entry_embeddings['error'] = str(e)\n",
    "        embeddings_list.append(entry_embeddings)\n",
    "\n",
    "# Final cleanup\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Save embeddings\n",
    "print(f\"Saving embeddings to: {save_path}\")\n",
    "with open(save_path, 'wb') as f:\n",
    "    pickle.dump(embeddings_list, f)\n",
    "\n",
    "print(f\"Processing complete. Total entries processed: {len(embeddings_list)}\")\n",
    "\n",
    "# Print summary\n",
    "successful_image_embeddings = sum(1 for entry in embeddings_list if entry.get('image_embedding') is not None)\n",
    "successful_text_embeddings = sum(1 for entry in embeddings_list if entry.get('text_embeddings') and len(entry['text_embeddings']) > 0)\n",
    "\n",
    "print(f\"Successful image embeddings: {successful_image_embeddings}/{len(embeddings_list)}\")\n",
    "print(f\"Successful text embeddings: {successful_text_embeddings}/{len(embeddings_list)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e45b8cc",
   "metadata": {},
   "source": [
    "## Multilingual CLIP Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f888ea3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multilingual CLIP imports\n",
    "from multilingual_clip import pt_multilingual_clip\n",
    "import transformers\n",
    "import open_clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ec5a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dropdown widget for Multilingual CLIP model selection\n",
    "mclip_model_options = [\n",
    "    (\"XLM-Roberta-Large-Vit-L-14\", \"M-CLIP/XLM-Roberta-Large-Vit-L-14\"),\n",
    "    (\"XLM-Roberta-Large-Vit-B-16Plus\", \"M-CLIP/XLM-Roberta-Large-Vit-B-16Plus\")\n",
    "]\n",
    "\n",
    "mclip_model_dropdown = widgets.Dropdown(\n",
    "    options=mclip_model_options,\n",
    "    value=\"M-CLIP/XLM-Roberta-Large-Vit-L-14\",\n",
    "    description='M-CLIP Model:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "def on_mclip_model_change(change):\n",
    "    global mclip_model_name, mclip_model_name_part\n",
    "    mclip_model_name = change['new']\n",
    "    mclip_model_name_part = mclip_model_name.split(\"/\")[-1]\n",
    "    print(f\"Selected M-CLIP model: {mclip_model_name}\")\n",
    "\n",
    "mclip_model_dropdown.observe(on_mclip_model_change, names='value')\n",
    "\n",
    "# Initialize variables\n",
    "mclip_model_name = mclip_model_dropdown.value\n",
    "mclip_model_name_part = mclip_model_name.split(\"/\")[-1]\n",
    "\n",
    "display(mclip_model_dropdown)\n",
    "print(f\"Initial M-CLIP model: {mclip_model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea4fdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Multilingual CLIP model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load text model\n",
    "mclip_model = pt_multilingual_clip.MultilingualCLIP.from_pretrained(mclip_model_name)\n",
    "mclip_tokenizer = transformers.AutoTokenizer.from_pretrained(mclip_model_name)\n",
    "\n",
    "# Load vision model\n",
    "vision_model, _, vision_preprocess = open_clip.create_model_and_transforms('ViT-B-16-plus-240', pretrained=\"laion400m_e32\")\n",
    "vision_model.to(device)\n",
    "\n",
    "print(f\"Multilingual CLIP model loaded: {mclip_model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9303af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset for Multilingual CLIP processing\n",
    "dataset = load_bias_benchmark_dataset(DATASET_NAME)\n",
    "\n",
    "eval_size = min(np.inf, len(dataset)) if END_INDEX is None else min(END_INDEX, len(dataset))\n",
    "eval_dataset = dataset.select(range(eval_size))\n",
    "\n",
    "embeddings_list = []\n",
    "\n",
    "# Setup save path\n",
    "save_path = os.path.join(EMBEDDING_SAVE_PATH, f\"image_text_embeddings_{mclip_model_name_part}_{START_INDEX}_{eval_size}.pkl\")\n",
    "\n",
    "print(f\"Processing entries from index {START_INDEX} to {eval_size}...\")\n",
    "\n",
    "# Process entries\n",
    "for i in tqdm(range(START_INDEX, min(eval_size, len(eval_dataset))), desc=\"Processing M-CLIP embeddings\"):\n",
    "    current_entry = eval_dataset[i]\n",
    "    \n",
    "    # Initialize entry with expected schema\n",
    "    entry_embeddings = {\n",
    "        'concept_id': current_entry.get('concept_id', None),\n",
    "        'concept_in_native': current_entry.get('concept_in_native'),\n",
    "        'image_id': current_entry.get('image_id', None),\n",
    "        'image_key': current_entry.get('image_id', None),\n",
    "        'index_in_dataset': i,\n",
    "        'concept': current_entry.get('concept', None),\n",
    "        'concept_country': current_entry.get('concept_country', None),\n",
    "        'country': current_entry.get('country', None),\n",
    "        'title': current_entry.get('title', None),\n",
    "        'image_embedding': None,\n",
    "        'text_embeddings': {}\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            # 1. Get image embedding\n",
    "            if current_entry.get('image') is not None:\n",
    "                image = vision_preprocess(current_entry['image']).unsqueeze(0).to(device)\n",
    "                image_embeddings = vision_model.encode_image(image)\n",
    "                entry_embeddings['image_embedding'] = image_embeddings.cpu().numpy()\n",
    "            else:\n",
    "                print(f\"Warning: No image found for entry index {i}\")\n",
    "            \n",
    "            # 2. Get text embeddings\n",
    "            if current_entry.get('concept'):\n",
    "                concept_text_embeddings = mclip_model.forward([current_entry['concept']], mclip_tokenizer)\n",
    "                entry_embeddings['text_embeddings']['concept_embedding'] = concept_text_embeddings.cpu().numpy()\n",
    "            \n",
    "            if current_entry.get('concept_in_native'):\n",
    "                native_embeddings = mclip_model.forward([current_entry['concept_in_native']], mclip_tokenizer)\n",
    "                entry_embeddings['text_embeddings']['translated_concept_embedding'] = native_embeddings.cpu().numpy()\n",
    "                \n",
    "        embeddings_list.append(entry_embeddings)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing entry {i}: {str(e)}\")\n",
    "        entry_embeddings['error'] = str(e)\n",
    "        embeddings_list.append(entry_embeddings)\n",
    "\n",
    "# Final cleanup\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Save embeddings\n",
    "print(f\"Saving embeddings to: {save_path}\")\n",
    "with open(save_path, 'wb') as f:\n",
    "    pickle.dump(embeddings_list, f)\n",
    "\n",
    "print(f\"Processing complete. Total entries processed: {len(embeddings_list)}\")\n",
    "\n",
    "# Print summary\n",
    "successful_image_embeddings = sum(1 for entry in embeddings_list if entry.get('image_embedding') is not None)\n",
    "successful_text_embeddings = sum(1 for entry in embeddings_list if entry.get('text_embeddings') and len(entry['text_embeddings']) > 0)\n",
    "\n",
    "print(f\"Successful image embeddings: {successful_image_embeddings}/{len(embeddings_list)}\")\n",
    "print(f\"Successful text embeddings: {successful_text_embeddings}/{len(embeddings_list)}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
