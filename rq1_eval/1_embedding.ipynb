{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f4c673",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7581bdf",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ca6c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers datasets tqdm pandas colpali-engine clip multilingual-clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbfda8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "XM_IMAGE_PATH = \"\"  # Crossmodal-3600 Image folder path\n",
    "XM_JSON_PATH = \"\"   # Crossmodal-3600 JSON file path (captions.jsonl)\n",
    "EMBEDDING_SAVE_PATH = \"\"  # Path to save the embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7489010a",
   "metadata": {},
   "source": [
    "# Embeddings\n",
    "\n",
    "    expect schema\n",
    "    - image_key\n",
    "    - image_embedding\n",
    "    - text_embeddings\n",
    "        - caption_embedding_[lang]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f56067c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to load local dataset\n",
    "def load_local_crossmodal_data(image_path, json_path):\n",
    "    \"\"\"\n",
    "    Load Crossmodal-3600 dataset from local paths.\n",
    "    \n",
    "    Args:\n",
    "        image_path: Path to the folder containing images\n",
    "        json_path: Path to the captions.jsonl file\n",
    "    \n",
    "    Returns:\n",
    "        List of data entries with image and captions\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    \n",
    "    # Load captions from JSON file\n",
    "    with open(json_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            entry = json.loads(line)\n",
    "            data.append(entry)\n",
    "    \n",
    "    # Load images and match with captions\n",
    "    for entry in data:\n",
    "        image_key = entry['image/key']\n",
    "        image_file = os.path.join(image_path, f\"{image_key}.jpg\")\n",
    "        \n",
    "        if os.path.exists(image_file):\n",
    "            entry['image'] = Image.open(image_file).convert('RGB')\n",
    "        else:\n",
    "            print(f\"Warning: Image not found: {image_file}\")\n",
    "            entry['image'] = None\n",
    "    \n",
    "    return data\n",
    "\n",
    "print(\"Helper function loaded. Ready to process local data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7b8ef3",
   "metadata": {},
   "source": [
    "## gme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce771947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GME imports\n",
    "from transformers import AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018a34a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "model_name = \"Alibaba-NLP/gme-Qwen2-VL-7B-Instruct\"\n",
    "model_real_name = model_name.split(\"/\")[-1]\n",
    "\n",
    "# Initialize GME model\n",
    "gme = AutoModel.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map='cuda',\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Load local dataset\n",
    "print(f\"Loading local dataset from: {XM_IMAGE_PATH} and {XM_JSON_PATH}\")\n",
    "dataset = load_local_crossmodal_data(XM_IMAGE_PATH, XM_JSON_PATH)\n",
    "\n",
    "eval_size = min(np.inf, len(dataset))\n",
    "eval_dataset = dataset[:eval_size]\n",
    "\n",
    "# Processing configuration\n",
    "start_index = 0\n",
    "end_index = eval_size\n",
    "\n",
    "embeddings_list = []\n",
    "\n",
    "# Setup save path\n",
    "os.makedirs(EMBEDDING_SAVE_PATH, exist_ok=True)\n",
    "save_path = os.path.join(EMBEDDING_SAVE_PATH, f\"image_text_embeddings_{model_real_name}_{start_index}_{end_index}.pkl\")\n",
    "\n",
    "print(f\"Processing entries from index {start_index} to {end_index}...\")\n",
    "print(f\"Total dataset size: {len(eval_dataset)}\")\n",
    "\n",
    "# Process entries within the specified range\n",
    "for i in tqdm(range(start_index, min(end_index, len(eval_dataset))), desc=\"Processing entries\"):\n",
    "    current_entry = eval_dataset[i]\n",
    "\n",
    "    # Initialize entry with expected schema\n",
    "    entry_embeddings = {\n",
    "        'image_key': current_entry.get('image/key', None),\n",
    "        'image_embedding': None,\n",
    "        'text_embeddings': {}\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            # 1. Get image embedding\n",
    "            if current_entry.get('image') is not None:\n",
    "                image_embeddings = gme.get_image_embeddings(images=[current_entry['image']])\n",
    "                entry_embeddings['image_embedding'] = image_embeddings.cpu().numpy()\n",
    "            else:\n",
    "                print(f\"Warning: No image found for entry index {i}\")\n",
    "\n",
    "            # 2. Get text embeddings for all languages\n",
    "            if 'captions' in current_entry and isinstance(current_entry['captions'], dict):\n",
    "                for lang_code, captions_list in current_entry['captions'].items():\n",
    "                    if isinstance(captions_list, list) and captions_list:\n",
    "                        # Get embeddings for all captions in this language\n",
    "                        lang_text_embeddings = gme.get_text_embeddings(texts=captions_list)\n",
    "                        entry_embeddings['text_embeddings'][f'caption_embedding_{lang_code}'] = lang_text_embeddings.cpu().numpy()\n",
    "            \n",
    "            if not entry_embeddings['text_embeddings']:\n",
    "                print(f\"Warning: No valid text embeddings generated for entry index {i}\")\n",
    "\n",
    "        embeddings_list.append(entry_embeddings)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing entry {i}: {str(e)}\")\n",
    "        entry_embeddings['error'] = str(e)\n",
    "        embeddings_list.append(entry_embeddings)\n",
    "\n",
    "# Final cleanup\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Save all collected embeddings\n",
    "print(f\"Saving embeddings to: {save_path}\")\n",
    "with open(save_path, 'wb') as f:\n",
    "    pickle.dump(embeddings_list, f)\n",
    "\n",
    "print(f\"Processing complete. Embeddings saved to: {save_path}\")\n",
    "print(f\"Total entries processed: {len(embeddings_list)}\")\n",
    "\n",
    "# Print summary statistics\n",
    "successful_image_embeddings = sum(1 for entry in embeddings_list if entry.get('image_embedding') is not None)\n",
    "successful_text_embeddings = sum(1 for entry in embeddings_list if entry.get('text_embeddings') and len(entry['text_embeddings']) > 0)\n",
    "\n",
    "print(f\"Successful image embeddings: {successful_image_embeddings}/{len(embeddings_list)}\")\n",
    "print(f\"Successful text embeddings: {successful_text_embeddings}/{len(embeddings_list)}\")\n",
    "\n",
    "# Show sample of what was saved\n",
    "if embeddings_list:\n",
    "    print(\"\\nSample entry structure:\")\n",
    "    sample_entry = embeddings_list[0]\n",
    "    for key, value in sample_entry.items():\n",
    "        if key == 'text_embeddings' and isinstance(value, dict):\n",
    "            print(f\"  {key}: {list(value.keys())}\")\n",
    "        elif isinstance(value, np.ndarray):\n",
    "            print(f\"  {key}: numpy array shape {value.shape}\")\n",
    "        else:\n",
    "            print(f\"  {key}: {type(value)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3889db3a",
   "metadata": {},
   "source": [
    "## colqwen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812fa5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ColQwen imports\n",
    "from colpali_engine.models import ColQwen2_5, ColQwen2_5_Processor, ColQwen2, ColQwen2Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5119ed9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b204cad121e24ef98f05fb0b36584435",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Model:', index=2, options=(('ColQwen2.5-7b-multilingual-v1.0', 'Metric-AI/ColQwen2.5-7b-â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial model: vidore/colqwen2-v1.0\n",
      "Initial model name part: colqwen2-v1.0\n"
     ]
    }
   ],
   "source": [
    "# Create a dropdown widget for model selection\n",
    "model_options = [\n",
    "    (\"ColQwen2.5-7b-multilingual-v1.0\", \"Metric-AI/ColQwen2.5-7b-multilingual-v1.0\"),\n",
    "    (\"colqwen2.5-v0.2\", \"vidore/colqwen2.5-v0.2\"), \n",
    "    (\"colqwen2-v1.0\", \"vidore/colqwen2-v1.0\")\n",
    "]\n",
    "\n",
    "model_dropdown = widgets.Dropdown(\n",
    "    options=model_options,\n",
    "    value=\"vidore/colqwen2-v1.0\",  # default selection\n",
    "    description='Model:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "def on_model_change(change):\n",
    "    global model_name, model_name_part\n",
    "    model_name = change['new']\n",
    "    model_name_part = model_name.split(\"/\")[-1]\n",
    "    print(f\"Selected model: {model_name}\")\n",
    "    print(f\"Model name part: {model_name_part}\")\n",
    "\n",
    "model_dropdown.observe(on_model_change, names='value')\n",
    "\n",
    "# Initialize the variables with the default selection\n",
    "model_name = model_dropdown.value\n",
    "model_name_part = model_name.split(\"/\")[-1]\n",
    "\n",
    "display(model_dropdown)\n",
    "print(f\"Initial model: {model_name}\")\n",
    "print(f\"Initial model name part: {model_name_part}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c5f4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"colqwen2.5\" in model_name:\n",
    "    processor = ColQwen2_5_Processor.from_pretrained(model_name)\n",
    "    model = ColQwen2_5.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "else:\n",
    "    processor = ColQwen2Processor.from_pretrained(model_name)\n",
    "    model = ColQwen2.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799a961b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load local dataset\n",
    "print(f\"Loading local dataset from: {XM_IMAGE_PATH} and {XM_JSON_PATH}\")\n",
    "dataset = load_local_crossmodal_data(XM_IMAGE_PATH, XM_JSON_PATH)\n",
    "\n",
    "eval_size = min(np.inf, len(dataset))\n",
    "eval_dataset = dataset[:eval_size]\n",
    "\n",
    "# Define the start and end indices for processing\n",
    "start_index = 0\n",
    "end_index = eval_size\n",
    "\n",
    "embeddings_list = []\n",
    "\n",
    "# Setup save path\n",
    "os.makedirs(EMBEDDING_SAVE_PATH, exist_ok=True)\n",
    "save_path = os.path.join(EMBEDDING_SAVE_PATH, f\"image_text_embeddings_{model_name_part}_{start_index}_{end_index}.pkl\")\n",
    "\n",
    "print(f\"Processing entries from index {start_index} to {end_index}...\")\n",
    "\n",
    "# Process entries within the specified range\n",
    "for i in tqdm(range(start_index, min(end_index, len(eval_dataset))), desc=\"Processing entries\"):\n",
    "    current_entry = eval_dataset[i]\n",
    "\n",
    "    # Initialize entry with expected schema\n",
    "    entry_embeddings = {\n",
    "        'image_key': current_entry.get('image/key', None),\n",
    "        'image_embedding': None,\n",
    "        'text_embeddings': {}\n",
    "    }\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # 1. Get image embedding\n",
    "        if current_entry.get('image') is not None:\n",
    "            batch_images = processor.process_images([current_entry['image']]).to(model.device)\n",
    "            image_embeddings = model(**batch_images)\n",
    "            entry_embeddings['image_embedding'] = image_embeddings.float().cpu().numpy()\n",
    "\n",
    "        # 2. Get text embeddings for ALL languages\n",
    "        if 'captions' in current_entry and isinstance(current_entry['captions'], dict):\n",
    "            for lang_code, captions_list in current_entry['captions'].items():\n",
    "                if isinstance(captions_list, list) and captions_list:\n",
    "                    # Process all captions for this language\n",
    "                    lang_embeddings = []\n",
    "                    for caption in captions_list:\n",
    "                        batch_caption = processor.process_queries([caption]).to(model.device)\n",
    "                        caption_embedding = model(**batch_caption)\n",
    "                        lang_embeddings.append(caption_embedding.float().cpu().numpy())\n",
    "                    \n",
    "                    # Store embeddings\n",
    "                    entry_embeddings['text_embeddings'][f'caption_embedding_{lang_code}'] = lang_embeddings\n",
    "                else:\n",
    "                    print(f\"Warning: No valid captions found for language '{lang_code}' in entry index {i}\")\n",
    "        else:\n",
    "            print(f\"Warning: 'captions' field not found or not a dictionary for entry index {i}\")\n",
    "\n",
    "        embeddings_list.append(entry_embeddings)\n",
    "\n",
    "    # Clean up GPU memory\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Save all collected embeddings\n",
    "with open(save_path, 'wb') as f:\n",
    "    pickle.dump(embeddings_list, f)\n",
    "\n",
    "print(f\"Processing complete. Embeddings saved to: {save_path}\")\n",
    "print(f\"Total entries processed: {len(embeddings_list)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4a9236",
   "metadata": {},
   "source": [
    "## jina"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debff08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jina imports\n",
    "from transformers import AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8125cc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def concatenate_tensors(tensor_list):\n",
    "    \"\"\"\n",
    "    Concatenates a list of tensors along dimension 0.\n",
    "    \n",
    "    Args:\n",
    "        tensor_list: A list of PyTorch tensors.\n",
    "    \n",
    "    Returns:\n",
    "        A single PyTorch tensor concatenated along dimension 0.\n",
    "        If the input list contains only one tensor, that tensor is returned.\n",
    "    \"\"\"\n",
    "    if len(tensor_list) == 1:\n",
    "        return tensor_list[0]\n",
    "    buffed_list = []\n",
    "    for i, tensor in enumerate(tensor_list):\n",
    "        buffed_list.append(tensor.unsqueeze(0))\n",
    "    return torch.cat(buffed_list, dim=0)\n",
    "\n",
    "# --- Configuration ---\n",
    "model_name = 'jinaai/jina-embeddings-v4'\n",
    "model_real_name = model_name.split(\"/\")[-1]\n",
    "\n",
    "# Load Model\n",
    "model = AutoModel.from_pretrained(model_name, trust_remote_code=True, torch_dtype=torch.float16)\n",
    "model.to(\"cuda\")\n",
    "\n",
    "# Load local dataset\n",
    "print(f\"Loading local dataset from: {XM_IMAGE_PATH} and {XM_JSON_PATH}\")\n",
    "dataset = load_local_crossmodal_data(XM_IMAGE_PATH, XM_JSON_PATH)\n",
    "\n",
    "eval_size = min(np.inf, len(dataset))\n",
    "eval_dataset = dataset[:eval_size]\n",
    "\n",
    "# Define the start and end indices for processing\n",
    "start_index = 0\n",
    "end_index = eval_size\n",
    "\n",
    "embeddings_list = []\n",
    "\n",
    "# Setup save path\n",
    "os.makedirs(EMBEDDING_SAVE_PATH, exist_ok=True)\n",
    "save_path = os.path.join(EMBEDDING_SAVE_PATH, f\"image_text_embeddings_{model_real_name}_{start_index}_{end_index}.pkl\")\n",
    "\n",
    "print(f\"Processing entries from index {start_index} to {end_index}...\")\n",
    "\n",
    "# Process entries within the specified range\n",
    "for i in tqdm(range(start_index, min(end_index, len(eval_dataset))), desc=\"Processing entries\"):\n",
    "    current_entry = eval_dataset[i]\n",
    "\n",
    "    # Initialize entry with expected schema\n",
    "    entry_embeddings = {\n",
    "        'image_key': current_entry.get('image/key', None),\n",
    "        'image_embedding': None,\n",
    "        'text_embeddings': {}\n",
    "    }\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # 1. Get image embedding\n",
    "        if current_entry.get('image') is not None:\n",
    "            image_embeddings = model.encode_image(\n",
    "                images=current_entry['image'],\n",
    "                task=\"retrieval\",\n",
    "            ).unsqueeze(0)\n",
    "            entry_embeddings['image_embedding'] = image_embeddings.cpu().numpy()\n",
    "\n",
    "        # 2. Get text embeddings for ALL languages\n",
    "        if 'captions' in current_entry and isinstance(current_entry['captions'], dict):\n",
    "            for lang_code, captions_list in current_entry['captions'].items():\n",
    "                if isinstance(captions_list, list) and captions_list:\n",
    "                    lang_text_embeddings = model.encode_text(\n",
    "                        texts=captions_list,\n",
    "                        task=\"retrieval\",\n",
    "                        prompt_name=\"query\",\n",
    "                    )\n",
    "                    lang_text_embeddings = concatenate_tensors(lang_text_embeddings)\n",
    "                    entry_embeddings['text_embeddings'][f'caption_embedding_{lang_code}'] = lang_text_embeddings.cpu().numpy()\n",
    "                else:\n",
    "                    print(f\"Warning: No valid captions found for language '{lang_code}' in entry index {i}\")\n",
    "        else:\n",
    "            print(f\"Warning: 'captions' field not found or not a dictionary for entry index {i}\")\n",
    "\n",
    "        embeddings_list.append(entry_embeddings)\n",
    "\n",
    "    # Clean up GPU memory\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Save all collected embeddings\n",
    "with open(save_path, 'wb') as f:\n",
    "    pickle.dump(embeddings_list, f)\n",
    "\n",
    "print(f\"Processing complete. Embeddings saved to: {save_path}\")\n",
    "print(f\"Total entries processed: {len(embeddings_list)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2824f2",
   "metadata": {},
   "source": [
    "## mclip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00550af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# M-CLIP imports\n",
    "from transformers import AutoModel\n",
    "from multilingual_clip import pt_multilingual_clip\n",
    "import transformers\n",
    "import clip\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# --- Configuration ---\n",
    "model_name = 'M-CLIP/XLM-Roberta-Large-Vit-L-14'\n",
    "model_real_name = model_name.split(\"/\")[-1]\n",
    "\n",
    "# Load Model & Tokenizer\n",
    "model = pt_multilingual_clip.MultilingualCLIP.from_pretrained(model_name).cuda()\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "vision_model, vision_preprocess = clip.load(\"ViT-L/14\", device=device)\n",
    "\n",
    "# Load local dataset\n",
    "print(f\"Loading local dataset from: {XM_IMAGE_PATH} and {XM_JSON_PATH}\")\n",
    "dataset = load_local_crossmodal_data(XM_IMAGE_PATH, XM_JSON_PATH)\n",
    "\n",
    "eval_size = min(np.inf, len(dataset))\n",
    "eval_dataset = dataset[:eval_size]\n",
    "\n",
    "# Define the start and end indices for processing\n",
    "start_index = 0\n",
    "end_index = eval_size\n",
    "\n",
    "embeddings_list = []\n",
    "\n",
    "# Setup save path\n",
    "os.makedirs(EMBEDDING_SAVE_PATH, exist_ok=True)\n",
    "save_path = os.path.join(EMBEDDING_SAVE_PATH, f\"image_text_embeddings_{model_real_name}_{start_index}_{end_index}.pkl\")\n",
    "\n",
    "print(f\"Processing entries from index {start_index} to {end_index}...\")\n",
    "\n",
    "# Process entries within the specified range\n",
    "for i in tqdm(range(start_index, min(end_index, len(eval_dataset))), desc=\"Processing entries\"):\n",
    "    current_entry = eval_dataset[i]\n",
    "\n",
    "    # Initialize entry with expected schema\n",
    "    entry_embeddings = {\n",
    "        'image_key': current_entry.get('image/key', None),\n",
    "        'image_embedding': None,\n",
    "        'text_embeddings': {}\n",
    "    }\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # 1. Get image embedding\n",
    "        if current_entry.get('image') is not None:\n",
    "            image = vision_preprocess(current_entry['image']).unsqueeze(0).to(device)\n",
    "            image_embeddings = vision_model.encode_image(image)\n",
    "            entry_embeddings['image_embedding'] = image_embeddings.cpu().numpy()\n",
    "\n",
    "        # 2. Get text embeddings for ALL languages\n",
    "        if 'captions' in current_entry and isinstance(current_entry['captions'], dict):\n",
    "            for lang_code, captions_list in current_entry['captions'].items():\n",
    "                if isinstance(captions_list, list) and captions_list:\n",
    "                    lang_text_embeddings = model.forward(captions_list, tokenizer)\n",
    "                    entry_embeddings['text_embeddings'][f'caption_embedding_{lang_code}'] = lang_text_embeddings.cpu().numpy()\n",
    "                else:\n",
    "                    print(f\"Warning: No valid captions found for language '{lang_code}' in entry index {i}\")\n",
    "        else:\n",
    "            print(f\"Warning: 'captions' field not found or not a dictionary for entry index {i}\")\n",
    "\n",
    "        embeddings_list.append(entry_embeddings)\n",
    "\n",
    "    # Clean up GPU memory\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Save all collected embeddings\n",
    "with open(save_path, 'wb') as f:\n",
    "    pickle.dump(embeddings_list, f)\n",
    "\n",
    "print(f\"Processing complete. Embeddings saved to: {save_path}\")\n",
    "print(f\"Total entries processed: {len(embeddings_list)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
